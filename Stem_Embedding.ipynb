{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Stem_Embedding.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1rSJe1XK0YlrzZ1bPcGHMSbKdSuz2icEK","authorship_tag":"ABX9TyMxkLs2cA3F6jN+3vlZbRmf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Q7UvvhZ85cXg"},"source":["import csv\n","import re\n","from gensim.models import Word2Vec\n","import pandas as pd \n","import numpy as np\n","from sklearn.datasets import load_files\n","from collections import Counter \n","from nltk.tokenize import word_tokenize\n","import gc\n","from nltk.stem.snowball import SnowballStemmer\n"," \n","from joblib import Parallel, delayed\n","import multiprocessing\n","import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1tKo-hq6bYe"},"source":["path = \"../data/articles1.csv\"\n","News = pd.read_csv(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"UnSDKA7K7hpE","executionInfo":{"status":"ok","timestamp":1617909662418,"user_tz":-120,"elapsed":704,"user":{"displayName":"pfe 2020","photoUrl":"","userId":"12668971617837840329"}},"outputId":"a3aab343-f970-4abf-eba6-8388e61b7d84"},"source":["News.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>publication</th>\n","      <th>author</th>\n","      <th>date</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>url</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>17283</td>\n","      <td>House Republicans Fret About Winning Their Hea...</td>\n","      <td>New York Times</td>\n","      <td>Carl Hulse</td>\n","      <td>2016-12-31</td>\n","      <td>2016.0</td>\n","      <td>12.0</td>\n","      <td>NaN</td>\n","      <td>WASHINGTON  —   Congressional Republicans have...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>17284</td>\n","      <td>Rift Between Officers and Residents as Killing...</td>\n","      <td>New York Times</td>\n","      <td>Benjamin Mueller and Al Baker</td>\n","      <td>2017-06-19</td>\n","      <td>2017.0</td>\n","      <td>6.0</td>\n","      <td>NaN</td>\n","      <td>After the bullet shells get counted, the blood...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>17285</td>\n","      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n","      <td>New York Times</td>\n","      <td>Margalit Fox</td>\n","      <td>2017-01-06</td>\n","      <td>2017.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>17286</td>\n","      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n","      <td>New York Times</td>\n","      <td>William McDonald</td>\n","      <td>2017-04-10</td>\n","      <td>2017.0</td>\n","      <td>4.0</td>\n","      <td>NaN</td>\n","      <td>Death may be the great equalizer, but it isn’t...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>17287</td>\n","      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n","      <td>New York Times</td>\n","      <td>Choe Sang-Hun</td>\n","      <td>2017-01-02</td>\n","      <td>2017.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0     id  ... url                                            content\n","0           0  17283  ... NaN  WASHINGTON  —   Congressional Republicans have...\n","1           1  17284  ... NaN  After the bullet shells get counted, the blood...\n","2           2  17285  ... NaN  When Walt Disney’s “Bambi” opened in 1942, cri...\n","3           3  17286  ... NaN  Death may be the great equalizer, but it isn’t...\n","4           4  17287  ... NaN  SEOUL, South Korea  —   North Korea’s leader, ...\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"Huxwd6an7j_e"},"source":["text= News['content']\n","X=text.values.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PmWyoZDe7m-k"},"source":["def majid(X):\n","    corpus = []\n","    for i in range(0, len(X)):\n","        #review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(X[i])) #remove punctuation\n","        review = re.sub(r'\\d+',' ', str(X[i]))# remove number\n","        review = review.lower() #lower case\n","        review = re.sub(r'\\s+', ' ', review) #remove extra space\n","        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n","        review = re.sub(r'\\s+', ' ', review) #remove spaces\n","        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n","        review = re.sub(r'\\s+$', '', review) #remove space from the end\n","        corpus.append(review)        \n","#    return corpus        \n","    #Tokenizing and Word Count  \n","    words=[]\n","    for i in range(len(corpus)):\n","        words= nltk.word_tokenize(corpus[i])\n","        #sentences.append(words)\n","   \n","    return words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"naJiehzF7tXA","executionInfo":{"status":"ok","timestamp":1617910133742,"user_tz":-120,"elapsed":406443,"user":{"displayName":"pfe 2020","photoUrl":"","userId":"12668971617837840329"}},"outputId":"5bb68bc5-9f19-431b-d755-8c7e446d9eff"},"source":["\n","X = [[el] for el in X] \n","\n","from joblib import Parallel, delayed\n","import multiprocessing\n","\n","num_cores = multiprocessing.cpu_count()\n","sentences = Parallel(n_jobs=num_cores)(delayed(majid)(i) for i in X)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A7JPRFhi8BjV"},"source":["\n","def stemming2(sentences):\n","        sno = nltk.stem.SnowballStemmer('english')\n","        stemmed_words = [sno.stem(word) for word in sentences]\n","        return stemmed_words\n","\n","def majid2(X):\n","    sentences= stemming2(X)\n","    gc.collect()\n","    return sentences\n","\n","\n","    \n","num_cores = multiprocessing.cpu_count()\n","sent2 = Parallel(n_jobs=num_cores)(delayed(majid2)(i) for i in sentences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6vF8V2YF7v1l","executionInfo":{"status":"ok","timestamp":1617912628046,"user_tz":-120,"elapsed":870452,"user":{"displayName":"pfe 2020","photoUrl":"","userId":"12668971617837840329"}},"outputId":"93f713c6-d18e-4e7e-9036-5497029da03c"},"source":["model1 = Word2Vec(sent2, min_count=5,size= 300,workers=multiprocessing.cpu_count(), window =1, sg = 0)\n","print('Done Training')\n","\n","SizeOfVocab = model1.wv.vocab\n","print('Size of Vocabulary=',len(SizeOfVocab))\n","print('Done making the Vocabulary')\n","\n","#####\n","model2 = Word2Vec(sent2, min_count=5,size= 300,workers=multiprocessing.cpu_count(), window =1, sg = 1)\n","print('Done Training')\n","\n","SizeOfVocab = model2.wv.vocab\n","print('Size of Vocabulary=',len(SizeOfVocab))\n","print('Done making the Vocabulary')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done Training\n","Size of Vocabulary= 47757\n","Done making the Vocabulary\n","Done Training\n","Size of Vocabulary= 47757\n","Done making the Vocabulary\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F6X3BNlqDjqt","executionInfo":{"status":"ok","timestamp":1617920684391,"user_tz":-120,"elapsed":48866,"user":{"displayName":"pfe 2020","photoUrl":"","userId":"12668971617837840329"}},"outputId":"0bbba5c6-4332-47c1-b888-8faa2fb016b1"},"source":["from gensim.models import Word2Vec, KeyedVectors   \n","\n","path = \"../Embeddings/\"\n","#model1.wv.save_word2vec_format('/content/drive/MyDrive/NLP/W-CBOW-Stem.bin.gz', binary=True)\n","model1.wv.save_word2vec_format(path + 'W-CBOW-Stem.txt', binary=False)\n","model1.save('W-CBOW-Stem.bin')\n","print('Done Saving Model1')\n","#####\n","#model2.wv.save_word2vec_format('/content/drive/MyDrive/NLP/W-Skip-Stem.gz', binary=True)\n","model2.wv.save_word2vec_format(path + 'W-Skip-Stem.txt', binary=False)\n","model2.save('W-Skip-Stem.bin')\n","print('Done Saving Model2')\n","\n","#model.save('model2.bin')\n","\n","print('Done Saving the Embeddings')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done Saving Model1\n","Done Saving Model2\n","Done Saving the Embeddings\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TMC7Kl2UG6wK"},"source":[""],"execution_count":null,"outputs":[]}]}