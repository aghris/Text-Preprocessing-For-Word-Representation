{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Punctuation_Embedding.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUMvGILuR2dC","outputId":"2e9179b8-adfe-42d4-be81-344b3b5a51fd"},"source":["import csv\n","import re\n","from gensim.models import Word2Vec\n","import pandas as pd \n","import numpy as np\n","from sklearn.datasets import load_files\n","from collections import Counter \n","from nltk.tokenize import word_tokenize\n","import gc\n","from nltk.stem.snowball import SnowballStemmer\n"," \n","from joblib import Parallel, delayed\n","import multiprocessing\n","import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRXfI-vJlN7O","outputId":"bcfc7cef-8fc6-486c-ce5a-fa6015cdcabd"},"source":["import nltk\n","from nltk.corpus import stopwords\n","example_sent = \"\"\"This is a sample sentence,extremely\"\"\"\n","  \n","stop_words = set(stopwords.words('english')) \n","  \n","word_tokens = word_tokenize(example_sent) \n","  \n","filtered_sentence = [w for w in word_tokens if not w in stop_words] \n","  \n","filtered_sentence"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This', 'sample', 'sentence', ',', 'extremely']"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"oHmfsol3Sdgo"},"source":["def majid(X,Punc=False):\n","    corpus = []\n","    for i in range(0, len(X)):\n","        if Punc:\n","          review = re.sub(r'[@%\\\\*=()/~#&\\+รก?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(X[i])) #remove punctuation\n","        review = re.sub(r'\\d+',' ', str(X[i]))# remove number\n","        review = review.lower() #lower case\n","        review = re.sub(r'\\s+', ' ', review) #remove extra space\n","        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n","        review = re.sub(r'\\s+', ' ', review) #remove spaces\n","        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n","        review = re.sub(r'\\s+$', '', review) #remove space from the end\n","        corpus.append(review)             \n","    #Tokenizing and Word Count  \n","    words=[]\n","    for i in range(len(corpus)):\n","        words= nltk.word_tokenize(corpus[i])\n","        #sentences.append(words)\n","    return words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzm4oOaKSgTe"},"source":["path = \"../data/articles1.csv\"\n","News = pd.read_csv(path)\n","text= News['content']\n","X=text.values.tolist()\n","X = [[el] for el in X] \n","num_cores = multiprocessing.cpu_count()\n","sentences = Parallel(n_jobs=num_cores)(delayed(majid)(i,True) for i in tqdm(X))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zRvwww9CSh0h","outputId":"6a822e0e-f6ca-4211-ec08-7966e7ff0bbb"},"source":["\n","\"\"\"\"\"\n","Making Vocabulary and Training the Model\n","(sg=0 CBOW , sg=1 Skip-gram)\n","\"\"\"\"\"\n","#########\n","\n","CBOW = Word2Vec(sentences, min_count=3,size= 300,workers=multiprocessing.cpu_count(), window =1, sg = 0)\n","print('Done Training')\n","\n","SizeOfVocab = CBOW.wv.vocab\n","print('Size of Vocabulary=',len(SizeOfVocab))\n","print('Done making the Vocabulary')\n","\n","#####\n","Skip_gram = Word2Vec(sentences, min_count=3,size= 300,workers=multiprocessing.cpu_count(), window =1, sg = 1)\n","print('Done Training')\n","\n","SizeOfVocab = Skip_gram.wv.vocab\n","print('Size of Vocabulary=',len(SizeOfVocab))\n","print('Done making the Vocabulary')\n","\n","\"\"\"\"\n","Saving the embeddings and the model\n","\"\"\"\"\"\n","\n","from gensim.models import Word2Vec, KeyedVectors   \n","#CBOW.wv.save_word2vec_format(path+'W-CBOW-POS.bin.gz', binary=True)\n","path = \"../Embeddings/\"\n","CBOW.wv.save_word2vec_format(path+'W-CBOW-POS.txt', binary=False)\n","CBOW.save(path+'W-CBOW-POS.bin')\n","print('Done Saving CBOW')\n","#####\n","#Skip_gram.wv.save_word2vec_format(path+'W-Skip-POS.gz', binary=True)\n","Skip_gram.wv.save_word2vec_format(path+'W-Skip-POS.txt', binary=False)\n","Skip_gram.save(path+'W-Skip-POS.bin')\n","print('Done Saving Skip_gram')\n","\n","print('Done Saving the Embeddings')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done Training\n","Size of Vocabulary= 88803\n","Done making the Vocabulary\n","Done Training\n","Size of Vocabulary= 88803\n","Done making the Vocabulary\n","Done Saving CBOW\n","Done Saving Skip_gram\n","Done Saving the Embeddings\n"],"name":"stdout"}]}]}