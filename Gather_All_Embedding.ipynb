{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gather_All_Embedding.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1K318gNM-liEoiRiAJFVZ3dKgbpr8-HQa","authorship_tag":"ABX9TyMqzHrepwZiRhaPS9qPEEAv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zCmQucT86rhE"},"source":["The objective of this notebook is to gather all preprocessing techniques in the same order cited in the paper, except the negation and spelling checker since we didn't neither find the words frequencies, nor the antonym file."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uMspFEyEpcoG","executionInfo":{"status":"ok","timestamp":1617921852758,"user_tz":-120,"elapsed":864,"user":{"displayName":"pfe 2020","photoUrl":"","userId":"12668971617837840329"}},"outputId":"3ac2b594-427a-4505-b792-dde7afd6a786"},"source":["from nltk.corpus import stopwords\n","import nltk\n","import re\n","from gensim.models import Word2Vec\n","import pandas as pd \n","import numpy as np\n","from gensim.models import Word2Vec, KeyedVectors   \n","import gc\n","from joblib import Parallel, delayed\n","import multiprocessing\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"aFpx2FDBprZz","executionInfo":{"status":"ok","timestamp":1617921832751,"user_tz":-120,"elapsed":8520,"user":{"displayName":"pfe 2020","photoUrl":"","userId":"12668971617837840329"}},"outputId":"72b275f9-e4b6-45d6-80a6-82362440d916"},"source":["path = \"../data/articles1.csv\"\n","News = pd.read_csv(path)\n","News.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>publication</th>\n","      <th>author</th>\n","      <th>date</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>url</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>17283</td>\n","      <td>House Republicans Fret About Winning Their Hea...</td>\n","      <td>New York Times</td>\n","      <td>Carl Hulse</td>\n","      <td>2016-12-31</td>\n","      <td>2016.0</td>\n","      <td>12.0</td>\n","      <td>NaN</td>\n","      <td>WASHINGTON  —   Congressional Republicans have...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>17284</td>\n","      <td>Rift Between Officers and Residents as Killing...</td>\n","      <td>New York Times</td>\n","      <td>Benjamin Mueller and Al Baker</td>\n","      <td>2017-06-19</td>\n","      <td>2017.0</td>\n","      <td>6.0</td>\n","      <td>NaN</td>\n","      <td>After the bullet shells get counted, the blood...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>17285</td>\n","      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n","      <td>New York Times</td>\n","      <td>Margalit Fox</td>\n","      <td>2017-01-06</td>\n","      <td>2017.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>17286</td>\n","      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n","      <td>New York Times</td>\n","      <td>William McDonald</td>\n","      <td>2017-04-10</td>\n","      <td>2017.0</td>\n","      <td>4.0</td>\n","      <td>NaN</td>\n","      <td>Death may be the great equalizer, but it isn’t...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>17287</td>\n","      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n","      <td>New York Times</td>\n","      <td>Choe Sang-Hun</td>\n","      <td>2017-01-02</td>\n","      <td>2017.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0     id  ... url                                            content\n","0           0  17283  ... NaN  WASHINGTON  —   Congressional Republicans have...\n","1           1  17284  ... NaN  After the bullet shells get counted, the blood...\n","2           2  17285  ... NaN  When Walt Disney’s “Bambi” opened in 1942, cri...\n","3           3  17286  ... NaN  Death may be the great equalizer, but it isn’t...\n","4           4  17287  ... NaN  SEOUL, South Korea  —   North Korea’s leader, ...\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"2mOOK7RdrBmC"},"source":["# Punctuation"]},{"cell_type":"code","metadata":{"id":"2Nz_8AbrrDel"},"source":["def unknown_punct(embed, punct):\n","    unknown = ''\n","    for p in punct:\n","        if p not in embed:\n","            unknown += p\n","            unknown += ' '\n","    return unknown\n","punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHi0AFtRrK59"},"source":["punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n","def clean_special_chars(text, punct, mapping):\n","    for p in mapping:\n","        text = text.replace(p, mapping[p])\n","    \n","    for p in punct:\n","        text = text.replace(p, f' {p} ')\n","    \n","    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n","    for s in specials:\n","        text = text.replace(s, specials[s])\n","    \n","    return text\n","\n","News['content'] = News['content'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TWvNcJnOrOpC"},"source":["text= News['content']\n","X=text.values.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"upBwj617rie0"},"source":["# Basic"]},{"cell_type":"code","metadata":{"id":"4A7S-hssp6oE"},"source":["\n","\n","def majid(X):\n","    corpus = []\n","    for i in range(0, len(X)):\n","        #review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(X[i])) #remove punctuation\n","        review = re.sub(r'\\d+','', str(X[i]))# remove number\n","        review = review.lower() #lower case\n","        review = re.sub(r'\\s+', ' ', review) #remove extra space\n","        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n","        review = re.sub(r'\\s+', ' ', review) #remove spaces\n","        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n","        review = re.sub(r'\\s+$', '', review) #remove space from the end\n","        corpus.append(review)        \n","#    return corpus        \n","    #Tokenizing and Word Count  \n","    words=[]\n","    for i in range(len(corpus)):\n","        words= nltk.word_tokenize(corpus[i])\n","    return words\n","\n","X = [[el] for el in X] \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bHlYhYglqA50"},"source":["num_cores = multiprocessing.cpu_count()\n","sentences = Parallel(n_jobs=num_cores)(delayed(majid)(i) for i in X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AGD463LCqTUE"},"source":["# Pos tagger"]},{"cell_type":"code","metadata":{"id":"855ng4FzqGps"},"source":["def pos_tagger(sentences):\n","    tags = [] #have the pos tag included\n","    nava_sen = []\n","    pt = nltk.pos_tag(sentences)\n","    nava = []\n","    nava_words = []\n","    for t in pt:\n","        if t[1].startswith('NN') or t[1].startswith('NNS') or t[1].startswith('NNP') or t[1].startswith('NNPS') or t[1].startswith('JJ') or t[1].startswith('JJR') or t[1].startswith('JJS') or  t[1].startswith('VB') or t[1].startswith('VBG') or t[1].startswith('VBN') or t[1].startswith('VBP') or t[1].startswith('VBZ') or t[1].startswith('RB') or t[1].startswith('RBR') or t[1].startswith('RBS'):\n","            nava.append(t)\n","            nava_words.append(t[0])\n","    return nava_words\n","\n","def majid2(X):\n","    review = pos_tagger(X)\n","    gc.collect()\n","    return review"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3uIs2YaqM7n"},"source":["\n","num_cores = multiprocessing.cpu_count()\n","sent_pos_tag = Parallel(n_jobs=num_cores)(delayed(majid2)(i) for i in sentences)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7o8IilgqWyD"},"source":["# Removing Stop Words"]},{"cell_type":"code","metadata":{"id":"8WlOQsHrqSnt"},"source":["def remove_stopwords(sentences):\n","        stopwords_list = nltk.corpus.stopwords.words('english')\n","        clean_words = [word for word in sentences if (word not in stopwords_list)] \n","        return clean_words \n","\n","def majid2(X):\n","    sentences = remove_stopwords(X)\n","    gc.collect()\n","    return sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T2Gq-HUSqdhu","executionInfo":{"status":"ok","timestamp":1617926237141,"user_tz":-120,"elapsed":985179,"user":{"displayName":"pfe 2020","photoUrl":"","userId":"12668971617837840329"}},"outputId":"ea0a59aa-6c27-4378-e888-069216d7ca9c"},"source":["\n","num_cores = multiprocessing.cpu_count()\n","sent_stop_word = Parallel(n_jobs=num_cores)(delayed(majid2)(i) for i in sent_pos_tag)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bKkA6wDCsHR9"},"source":["# Stemming"]},{"cell_type":"code","metadata":{"id":"3Wqt-D2ZsGju"},"source":["def stemming2(sentences):\n","        sno = nltk.stem.SnowballStemmer('english')\n","        stemmed_words = [sno.stem(word) for word in sentences]\n","        return stemmed_words\n","\n","def majid2(X):\n","    sentences = stemming2(X)\n","    gc.collect()\n","    return sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvXou42BsPcw"},"source":["num_cores = multiprocessing.cpu_count()\n","sent2 = Parallel(n_jobs=num_cores)(delayed(majid2)(i) for i in sent_stop_word)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i7iS1iz76_7L"},"source":["# CBOW and Skip Gram embeddings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOIZ1VPa61zb","executionInfo":{"status":"ok","timestamp":1617928477947,"user_tz":-120,"elapsed":490576,"user":{"displayName":"pfe 2020","photoUrl":"","userId":"12668971617837840329"}},"outputId":"fd5931e3-29ed-4a40-dc5a-b54ef210559d"},"source":["model1 = Word2Vec(sent2, min_count=3,size= 300,workers=multiprocessing.cpu_count(), window =1, sg = 0)\n","print('Done Training')\n","\n","SizeOfVocab = model1.wv.vocab\n","print('Size of Vocabulary=',len(SizeOfVocab))\n","print('Done making the Vocabulary')\n","\n","#####\n","model2 = Word2Vec(sent2, min_count=3,size= 300,workers=multiprocessing.cpu_count(), window =1, sg = 1)\n","print('Done Training')\n","\n","SizeOfVocab = model2.wv.vocab\n","print('Size of Vocabulary=',len(SizeOfVocab))\n","print('Done making the Vocabulary')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done Training\n","Size of Vocabulary= 59102\n","Done making the Vocabulary\n","Done Training\n","Size of Vocabulary= 59102\n","Done making the Vocabulary\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zHfjdlJN7FnZ"},"source":["# Save Results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"apQHi245B5Ut","executionInfo":{"status":"ok","timestamp":1617928549333,"user_tz":-120,"elapsed":30811,"user":{"displayName":"pfe 2020","photoUrl":"","userId":"12668971617837840329"}},"outputId":"8a1af3d6-6c97-4816-89f8-faf3826a0318"},"source":["path = \"../Embeddings/\"\n","model1.wv.save_word2vec_format(path + 'W-CBOW-ALL.txt', binary=False)\n","model1.save('W-CBOW-ALL.bin')\n","print('Done Saving Model1')\n","#####\n","model2.wv.save_word2vec_format(path + 'W-Skip-ALL.txt', binary=False)\n","model2.save('W-Skip-ALL.bin')\n","print('Done Saving Model2')\n","\n","#model.save('model2.bin')\n","\n","print('Done Saving the Embeddings')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done Saving Model1\n","Done Saving Model2\n","Done Saving the Embeddings\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lh2KQ50lCIqA"},"source":[""],"execution_count":null,"outputs":[]}]}