{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pos_Tag_Embedding.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8w6s7KQf0y0W","outputId":"72f14ace-0449-4ea6-b543-054e0b7576bc"},"source":["import csv\n","import re\n","from gensim.models import Word2Vec\n","import pandas as pd \n","import numpy as np\n","from sklearn.datasets import load_files\n","from collections import Counter \n","from nltk.tokenize import word_tokenize\n","import gc\n","from nltk.stem.snowball import SnowballStemmer\n"," \n","from joblib import Parallel, delayed\n","import multiprocessing\n","import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"markdown","metadata":{"id":"w68k7OdN1Nwr"},"source":["# Import Data"]},{"cell_type":"code","metadata":{"id":"iM4eYwWj-oqt"},"source":["def majid(X,Punc=False):\n","    corpus = []\n","    for i in range(0, len(X)):\n","        if Punc:\n","          review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(X[i])) #remove punctuation\n","        review = re.sub(r'\\d+',' ', str(X[i]))# remove number\n","        review = review.lower() #lower case\n","        review = re.sub(r'\\s+', ' ', review) #remove extra space\n","        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n","        review = re.sub(r'\\s+', ' ', review) #remove spaces\n","        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n","        review = re.sub(r'\\s+$', '', review) #remove space from the end\n","        corpus.append(review)             \n","    #Tokenizing and Word Count  \n","    words=[]\n","    for i in range(len(corpus)):\n","        words= nltk.word_tokenize(corpus[i])\n","        #sentences.append(words)\n","    return words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cUJS5YE_t6s"},"source":["path = \"../data/articles1.csv\"\n","News = pd.read_csv(path)\n","text= News['content']\n","X=text.values.tolist()\n","X = [[el] for el in X] \n","num_cores = multiprocessing.cpu_count()\n","sentences = Parallel(n_jobs=num_cores)(delayed(majid)(i) for i in X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1nAik8UPU_5"},"source":["def majid2(X,func):\n","    review = func(X)\n","    gc.collect()\n","    return review"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_37nPd3LBuQ"},"source":["### Post Tagging"]},{"cell_type":"code","metadata":{"id":"1xEbijdLPBYh"},"source":["def pos_tagger(sentences):\n","    tags = [] #have the pos tag included\n","    nava_sen = []\n","    pt = nltk.pos_tag(sentences)\n","    nava = []\n","    nava_words = []\n","    for t in pt:\n","        if t[1].startswith('NN') or t[1].startswith('NNS') or t[1].startswith('NNP') or t[1].startswith('NNPS') or t[1].startswith('JJ') or t[1].startswith('JJR') or t[1].startswith('JJS') or  t[1].startswith('VB') or t[1].startswith('VBG') or t[1].startswith('VBN') or t[1].startswith('VBP') or t[1].startswith('VBZ') or t[1].startswith('RB') or t[1].startswith('RBR') or t[1].startswith('RBS'):\n","            nava.append(t)\n","            nava_words.append(t[0])\n","    return nava_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Qcyyf9DK_gN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e64e738-884d-499c-e1f7-24c3ddefb4ca"},"source":["from tqdm import tqdm\n","num_cores = multiprocessing.cpu_count()\n","sent2 = Parallel(n_jobs=num_cores)(delayed(majid2)(i,pos_tagger) for i in tqdm(sentences))\n","count = 0\n","c = {}\n","for words in sent2:\n","  for s in words:\n","    if s in c:\n","        c[s] += 1\n","    else:\n","        c[s] = 1\n","    count+=1\n","d= []\n","\n","for k,v in c.items():\n","    if v == 1:\n","        d.append(k) \n","    \n","print('Corpus Size=', count)        \n","print('Unique words=', len(d)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 50000/50000 [43:48<00:00, 19.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Corpus Size= 21854171\n","Unique words= 69540\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"McP80MIEFE6_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0d54574-7228-4216-f0cd-2684eeb08cb4"},"source":["\n","\"\"\"\"\"\n","Making Vocabulary and Training the Model\n","(sg=0 CBOW , sg=1 Skip-gram)\n","\"\"\"\"\"\n","#########\n","#path = \"/content/drive/MyDrive/NLP/embeddings/\"\n","CBOW = Word2Vec(sentences, min_count=3,size= 300,workers=multiprocessing.cpu_count(), window =1, sg = 0)\n","print('Done Training')\n","\n","SizeOfVocab = CBOW.wv.vocab\n","print('Size of Vocabulary=',len(SizeOfVocab))\n","print('Done making the Vocabulary')\n","\n","#####\n","Skip_gram = Word2Vec(sentences, min_count=3,size= 300,workers=multiprocessing.cpu_count(), window =1, sg = 1)\n","print('Done Training')\n","\n","SizeOfVocab = Skip_gram.wv.vocab\n","print('Size of Vocabulary=',len(SizeOfVocab))\n","print('Done making the Vocabulary')\n","\n","\"\"\"\"\n","Saving the embeddings and the model\n","\"\"\"\"\"\n","\n","from gensim.models import Word2Vec, KeyedVectors   \n","#CBOW.wv.save_word2vec_format('W-CBOW-POS.bin.gz', binary=True)\n","path = \"../Embeddings/\"\n","CBOW.wv.save_word2vec_format(path + \"W-CBOW-POS.txt\", binary=False)\n","CBOW.save('W-CBOW-POS.bin')\n","print('Done Saving CBOW')\n","#####\n","#Skip_gram.wv.save_word2vec_format('W-Skip-POS.gz', binary=True)\n","path = \"../Embeddings/\"\n","Skip_gram.wv.save_word2vec_format(path + \"W-Skip-POS.txt\", binary=False)\n","Skip_gram.save('W-Skip-POS.bin')\n","print('Done Saving Skip_gram')\n","print('Done Saving the Embeddings')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done Training\n","Size of Vocabulary= 88803\n","Done making the Vocabulary\n","Done Training\n","Size of Vocabulary= 88803\n","Done making the Vocabulary\n","Done Saving CBOW\n","Done Saving Skip_gram\n","Done Saving the Embeddings\n"],"name":"stdout"}]}]}